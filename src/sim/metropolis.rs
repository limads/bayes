use nalgebra::*;
use crate::distr::*;
use super::*;
// use crate::optim::*;
// use std::ops::AddAssign;
use crate::distr::Estimator;

/// (Work in progress) The Metropolis-Hastings posterior sampler generate random walk draws
/// from a known proposal distribution (like a gaussian approximation generated by the
/// EM algorithm) and checks how the log-probability of this draw overestimate or underestimate
/// the unknown target posterior density. The size of the mismatch between the proposal and
/// the target distribution is used to build a decision rule to either re-sample at the current
/// position or move the position at which draws are made. After many iterations,
/// the accumulated samples generate a non-parametric representation
/// of the marginal posterior distribution, from which summary statistics can be calculated
/// by averaging over sufficiently spaced draws.
pub struct Metropolis<D>
    where
        D : Distribution
{

    _model : D,

    _proposal : MultiNormal

}

impl<D> Metropolis<D>
    where D : Distribution
{

    fn _step(&mut self) -> bool {
        // (1) Let the posterior have a natural vector order
        // (2) Initialize all parameters from a starting distribution
        // (3) For t = 0..T
        //      (3.1) Sample a proposal from a "jumping" distribution as J_t(theta*|theta_{t-1}) (Random walk increment).
        //      (3.2) Calculate the density ratio r = ( p(theta*|y) / J_t(theta*|theta t-1) ) / ( p(theta t-1|y) / J_t(theta t-1 | theta*) )
        //      (3.3) Set theta_t = theta* with probability min(r, 1); Set theta_t = theta_{t-1} otherwise. (Use uniform RNG over 0-1)
        unimplemented!()
    }

}

impl<D> Estimator<D> for Metropolis<D>
    where D : Distribution
{

    fn fit<'a>(&'a mut self, _y : DMatrix<f64>, x : Option<DMatrix<f64>>) -> Result<&'a D, &'static str> {
        /*let mut lp = 0.0;
        let f = |post_node : &mut dyn Posterior| {
            f.trajectory_mut().unwrap().step();
        };
        // setting the approximation at a likelihood node should also set
        // the approximation at all factors. Calling log_prob(.) over the
        // approximation returns the log_prob(.) of the approximation plus
        // of each approximation factor recursively.
        let mut approx_lp = 0.0
        let (left_fact, right_fact) = self.dyn_factors();
        if let Some(left_fact) = left_fact {
            approx_lp += left_fact.log_prob(y);
        }
        if let Some(right_fact) = rigth_fact {
            approx_lp += rigth_fact.log_prob(y);
        }
        let target_lp = self.log_prob(y);
        let metr_ratio = ...
        let mut pos = 0;
        let mut weights = DVector::from_element(max_iter, 0. as f64);
        let w_incr = 1. / max_iter as f64;
        for _ in 0..max_iter {
            if metr_ratio >= 1.0 {
                self.visit_factors(f);
                pos += 1;
            }
            weights[pos] += w_incr;
        }
        let weigths = weights.remove_columns(pos, weights.nrows() - pos);*/
        unimplemented!()
    }

}
