use super::*;
use rand_distr;
use rand;
use crate::fit::markov::*;
use std::default::Default;
use std::fmt::{self, Display};
use anyhow;
use serde_json::{self, Value, map::Map};
use crate::model::Model;
use std::convert::{TryFrom, TryInto};
use crate::model;
use argmin;
use either::Either;
use crate::fit::Estimator;
use crate::calc::Variate;
use std::mem;
use std::iter::FromIterator;

pub type BernoulliFactor = UnivariateFactor<Beta>;

/// The Bernoulli is the exponential-family distribution
/// used as the likelihood for binary outcomes. Each realization is parametrized
/// by a proportion parameter θ (0.0 ≥ θ ≥ 1.0), whose natural
/// parameter transformation is the logit ln(θ / (1.0 - θ)):
///
///<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
/// <semantics>
///  <mrow>
///   <mi>p</mi>
///   <mrow>
///    <mrow>
///     <mo fence="true" stretchy="false">(</mo>
///     <mrow>
///      <mrow>
///       <mi>y</mi>
///       <mo stretchy="false">∣</mo>
///       <mi>θ</mi>
///      </mrow>
///     </mrow>
///     <mo fence="true" stretchy="false">)</mo>
///    </mrow>
///    <mo stretchy="false">=</mo>
///    <msup>
///     <mi>θ</mi>
///     <mi>y</mi>
///    </msup>
///   </mrow>
///   <msup>
///    <mrow>
///     <mo fence="true" stretchy="false">(</mo>
///     <mrow>
///      <mrow>
///       <mn>1</mn>
///       <mo stretchy="false">−</mo>
///       <mi>θ</mi>
///      </mrow>
///     </mrow>
///     <mo fence="true" stretchy="false">)</mo>
///    </mrow>
///    <mrow>
///     <mn>1</mn>
///     <mo stretchy="false">−</mo>
///     <mi>y</mi>
///    </mrow>
///   </msup>
///  </mrow>
///  <annotation encoding="StarMath 5.0">p( y divides %theta ) = %theta^y ( 1 - %theta )^{ 1 - y }</annotation>
/// </semantics>
/// </math>
///
/// The sums of n realizations of a Bernoulli with probability θ is called a Binomial distribution
/// and approaches a Poison with lambda = nθ as n increases indefinitely.
/// # Example
///
/// ```
/// use bayes::distr::*;
///
/// let n = 1000;
/// let bern = Bernoulli::new(n, None);
/// let y = bern.sample();
///
/// // Maximum likelihood estimate
/// let mle = Bernoulli::mle((&y).into());
///
/// // Bayesian conjugate estimate
/// let mut bern_cond = bern.condition(Beta::new(1,1));
/// bern_cond.fit(y, None);
/// let post : Beta = bern_cond.take_factor().unwrap();
/// assert!(post.mean()[0] - mle.mean()[0] < 1E-3);
/// ```
#[derive(Debug, Clone)]
pub struct Bernoulli {

    theta : DVector<f64>,

    /// Log-prob argument when struct represent a conditional expectation.
    eta : DVector<f64>,

    factor : BernoulliFactor,

    //eta_traj : Option<Trajectory>,

    sampler : Vec<rand_distr::Bernoulli>,

    /// log-partition vector. Has one entry for each value of the eta vector. The log-partition
    /// is a non-linear function of the natural parameter that is subtracted from the eta*y term
    /// to complete the log-likelihood specification.
    log_part : DVector<f64>,

    /// In case this has a beta factor, when updating the parameter, also update this buffer
    /// with [log(theta) log(1-theta)]; and pass it instead of the eta as the log-prob argument.
    suf_theta : Option<DMatrix<f64>>,

    obs : Option<DMatrix<f64>>,
    
    // fixed_obs : Option<DMatrix<f64>>,
    
    name : Option<String>,
    
    fixed_names : Option<Vec<String>>,
    
    n : usize,

    sample : HashMap<String, Either<Vec<f64>, Vec<bool>>>

}

impl Bernoulli {

    /// logit informs if the constructor parameter
    /// and any incoming set_parameter calls should be interpreted as the natural parameter.
    /// Bernoullis parametrized by the logit cannot have conjugate beta factors; but can
    /// be conditioned on generic unconstrained continuous parameters such as the ones
    /// generated by a normal distribution.
    pub fn new(n : usize, theta : Option<f64>) -> Self {
        if let Some(t) = theta {
            assert!(t > 0.0 && t < 1.0);
        }
        let mut bern : Bernoulli = Default::default();
        bern.n = n;
        bern.log_part = DVector::zeros(n);
        let theta = DVector::from_element(n, theta.unwrap_or(0.5));
        bern.set_parameter(theta.rows(0,theta.nrows()), false);
        bern
    }

    pub fn from_sample(sample : &dyn Sample, name : &str) -> Option<Self> {
        match sample.variable(name) {
            Variable::Binary(b) => {
                let vars : Vec<_> = b.collect();
                let bern = Bernoulli::likelihood(vars.iter());
                Some(bern)
            },
            _ => None
        }
    }

    /*pub fn factorial(n: usize) -> usize {
        if n < 2 {
            1
        } else {
            n * Self::factorial(n - 1)
        }
    }*/

}

impl Conditional<Beta> for Bernoulli {

    fn condition(mut self, b : Beta) -> Bernoulli {
        self.factor = BernoulliFactor::Conjugate(b);
        self.suf_theta = Some(Beta::sufficient_stat(
            self.theta.slice((0, 0), (self.theta.nrows(), 1))
        ));
        // TODO update sampler vector
        self
    }

    fn view_factor(&self) -> Option<&Beta> {
        match &self.factor {
            BernoulliFactor::Conjugate(d) => Some(d),
            _ => None
        }
    }

    fn take_factor(self) -> Option<Beta> {
        match self.factor {
            BernoulliFactor::Conjugate(b) => Some(b),
            _ => None
        }
    }

    fn factor_mut(&mut self) -> Option<&mut Beta> {
        match &mut self.factor {
            BernoulliFactor::Conjugate(b) => Some(b),
            _ => None
        }
    }

}

impl Conditional<MultiNormal> for Bernoulli {

    fn condition(mut self, m : MultiNormal) -> Bernoulli {
        self.factor = BernoulliFactor::Fixed(m);
        // TODO Update samplers
        // TODO Update eta, mean, etc.
        self.suf_theta = None;
        self
    }

    fn view_factor(&self) -> Option<&MultiNormal> {
        match &self.factor {
            BernoulliFactor::Fixed(m) => Some(m),
            _ => None
        }
    }

    fn take_factor(self) -> Option<MultiNormal> {
        match self.factor {
            BernoulliFactor::Fixed(m) => Some(m),
            _ => None
        }
    }

    fn factor_mut(&mut self) -> Option<&mut MultiNormal> {
        match &mut self.factor {
            BernoulliFactor::Fixed(m) => Some(m),
            _ => None
        }
    }

}

impl ExponentialFamily<U1> for Bernoulli
    where
        Self : Distribution
{

    fn base_measure(y : DMatrixSlice<'_, f64>) -> DVector<f64> {
        DVector::from_element(y.nrows(), 1.)
    }

    fn sufficient_stat(y : DMatrixSlice<'_, f64>) -> DMatrix<f64> {
        assert!(y.ncols() == 1);
        DMatrix::from_element(1, 1, y.sum() / (y.nrows() as f64) )
    }

    fn suf_log_prob(&self, t : DMatrixSlice<'_, f64>) -> f64 {
        assert!(t.nrows() == 1 && t.ncols() == 1);
        assert!(self.eta.nrows() == 1);
        assert!(self.log_part.nrows() == 1);
        self.eta[0] * t[0] - self.log_part[0]
    }

    fn update_log_partition<'a>(&'a mut self, /*eta : DVectorSlice<'_, f64>*/ ) {
        if self.log_part.nrows() != self.eta.nrows() {
            self.log_part = DVector::zeros(self.eta.nrows());
        }
        self.log_part.iter_mut()
            .zip(self.eta.iter())
            .for_each(|(l,e)| { *l = (1. + e.exp()).ln(); } );
    }

    fn log_partition<'a>(&'a self) -> &'a DVector<f64> {
        &self.log_part
    }

    /*fn update_grad(&mut self, _eta : DVectorSlice<'_, f64>) {
        unimplemented!()
    }

    fn grad(&self) -> &DVector<f64> {
        unimplemented!()
    }*/

    fn link_inverse<S>(eta : &Matrix<f64, Dynamic, U1, S>) -> DVector<f64>
    where S : Storage<f64, Dynamic, U1>
    {
        eta.map(|e| e.sigmoid() )
    }

    fn link<S>(theta : &Matrix<f64, Dynamic, U1, S>) -> DVector<f64>
    where S : Storage<f64, Dynamic, U1>
    {
        theta.map(|p| p.logit() )
    }

}

impl Likelihood<bool> for Bernoulli {

    fn view_variables(&self) -> Option<Vec<String>> {
        self.name.as_ref().map(|name| vec![name.clone()] )
    }
    
    fn likelihood<'a>(obs : impl IntoIterator<Item=&'a bool>) -> Self {
        let mut bern = Bernoulli::new(1, None);
        bern.observe(obs);
        bern
    }

    fn observe<'a>(&mut self, obs : impl IntoIterator<Item=&'a bool>) {
        let cvt_obs : Vec<f64> = obs.into_iter().map(|val| if *val { 1.0 } else { 0.0 }).collect();
        observe_univariate_generic(self, cvt_obs.iter());
    }
    
    /*fn factors_mut<'a>(&'a mut self) -> (Option<&'a mut dyn Posterior>, Option<&'a mut dyn Posterior>) {
        (super::univariate_factor(&mut self.factor), None)
    }*/

    fn with_variables(&mut self, vars : &[&str]) -> &mut Self {
        assert!(vars.len() == 1);
        self.name = Some(vars[0].to_string());
        self
    }
    
    fn view_fixed(&self) -> Option<Vec<String>> {
        self.fixed_names.clone()
    }
    
    fn with_fixed(&mut self, fixed : &[&str]) -> &mut Self {
        self.fixed_names = Some(fixed.iter().map(|s| s.to_string()).collect());
        self
    }
    
    fn view_variable_values(&self) -> Option<&DMatrix<f64>> {
        self.obs.as_ref()
    }

    // fn view_fixed_values(&self) -> Option<&DMatrix<f64>> {
    //     self.fixed_obs.as_ref()
    // }

    fn observe_sample(&mut self, sample : &dyn Sample, vars : &[&str]) {
        //self.obs = Some(super::observe_univariate(self.name.clone(), self.theta.len(), self.obs.take(), sample));
        let mut obs = self.obs.take().unwrap_or(DMatrix::zeros(self.theta.len(), 1));
        // self.n = 0;
        if let Some(name) = vars.get(0) {
            if let Variable::Binary(col) = sample.variable(&name) {
                for (tgt, src) in obs.iter_mut().zip(col) {
                    if src {
                        *tgt = 1.0;
                    } else {
                        *tgt = 0.0;
                    }
                    // self.n += 1;
                }
            }
        }
        self.obs = Some(obs);
        
        /*if let Some(fixed_names) = &self.fixed_names {
            let fix_names = fixed_names.clone();
            super::observe_real_columns(&fix_names[..], sample, &mut self.fixed_obs, self.n);
        }*/
        
        //self
    }
    
    /*fn mle(y : DMatrixSlice<'_, f64>) -> Result<Self, anyhow::Error> {
        let prop = y.sum() / y.nrows() as f64;
        Ok(Self::new(1, Some(prop)))
    }*/

    /*fn mean_mle(y : DMatrixSlice<'_, f64>) -> f64 {
        assert!(y.ncols() == 1);
        let mle = y.iter().fold(0.0, |ys, y| {
            assert!(*y == 0. || *y == 1.); ys + y
        }) / (y.nrows() as f64);
        mle
    }

    fn var_mle(y : DMatrixSlice<'_, f64>) -> f64 {
        let m = Self::mean_mle(y);
        m * (1. - m)
    }*/

    /*fn visit_factors<F>(&mut self, f : F) where F : Fn(&mut dyn Posterior) {
        match self.factor {
            BernoulliFactor::Conjugate(ref mut b) => {
                f(b);
                b.visit_post_factors(&f as &dyn Fn(&mut dyn Posterior));
            },
            BernoulliFactor::Fixed(ref mut m) => {
                f(m);
                m.visit_post_factors(&f as &dyn Fn(&mut dyn Posterior));
            },
            _ => { }
        }
    }*/

   /*fn factors_mut(&mut self) -> Factors {
        unimplemented!()
        /*match &mut self.factor {
            BernoulliFactor::Conjugate(b) => {
                let factors = Factors::new_empty();
                let factors = factors.aggregate(b);
                b.aggregate_factors(factors)
                //if let (Some(opt_a), _) = factors.as_slice()[0].dyn_factors_mut() {
                //    unimplemented!()
                //} else {
                //    factors
                //}
            },
            BernoulliFactor::Fixed(m) => {
                unimplemented!()
            },
            _ => Factors::new_empty()
        }*/
        // f.map(|f| f.aggregate_factors(vec![f].into()) ).unwrap_or(Factors::new_empty())
        //if let Some(f) = factors.as_slice().get(0) {
        //    panic!("Unimplemented")
        //} else {
        //    factors
        //}

        //if let Some(f) = factors.as_slice().get(0) {
        //    f.aggregate_factors(factors)
        //} else {
        //    factors
        //}
        // let factors = f_vec.into();
        // factors
        //if let BernoulliFactor::Conjugate(ref mut b) =
        /*match &mut self.factor {
            BernoulliFactor::Conjugate(b) => {
                b.aggregate_factors(factors)
            },
            BernoulliFactor::Fixed(m) => {
                m.aggregate_factors(factors)
            },
            _ => Factors::new_empty()
        }*/
    }*/

}

impl<'a> Estimator<'a, &'a Beta> for Bernoulli {

    type Algorithm = ();

    type Error = &'static str;

    //fn predict<'a>(&'a self, cond : Option<&'a Sample/*<'a>*/>) -> Box<dyn Sample /*<'a>*/> {
    //    unimplemented!()
    //}
    
    /*fn take_posterior(self) -> Option<Beta> {
        unimplemented!()
    }
    
    fn view_posterior<'a>(&'a self) -> Option<&'a Beta> {
        unimplemented!()
    }*/
    
    fn fit(&'a mut self, algorithm : Option<()>) -> Result<&'a Beta, &'static str> {
        // self.observe_sample(sample);
        // assert!(y.ncols() == 1);
        // assert!(x.is_none());
        match self.factor {
            BernoulliFactor::Conjugate(ref mut beta) => {
                let y = self.obs.clone().unwrap();
                let n = y.nrows() as f64;
                let ys = y.column(0).sum();
                let (a, b) = (beta.view_parameter(false)[0], beta.view_parameter(false)[1]);
                let new_param = DVector::from_column_slice(&[a + ys, b + n - ys]);
                beta.set_parameter(new_param.rows(0, new_param.nrows()), false);
                Ok(&(*beta))
            },
            _ => Err("Distribution does not have a conjugate factor")
        }
    }

}

/// Implementation for logistic regression
impl<'a> Estimator<'a, &'a MultiNormal> for Bernoulli {

    type Algorithm = ();

    type Error = &'static str;

    /// Runs the inference algorithm for the informed sample matrix,
    /// returning a reference to the modified model (from which
    /// the posterior information of interest can be retrieved).
    fn fit(&'a mut self, algorithm : Option<Self::Algorithm>) -> Result<&'a MultiNormal, &'static str> {
        unimplemented!()
    }

}

impl Distribution for Bernoulli
    where Self : Sized
{

    fn sample(&self, dst : &mut [f64]) {
        use rand_distr::{Distribution};
        let opt_theta = sample_canonical_factor::<Bernoulli,_>(self.view_fixed_values(), &self.factor, self.n);
        let theta = opt_theta.as_ref().unwrap_or(&self.theta);
        for (i, _) in theta.iter().enumerate() {
            dst[i] = (self.sampler[i].sample(&mut rand::thread_rng()) as i32) as f64;
        }
    }

    fn set_natural<'a>(&'a mut self, new_eta : &'a mut dyn Iterator<Item=&'a f64>) {
        let (eta, theta) = (&mut self.eta, &mut self.theta);
        *eta = DVector::from(Vec::from_iter(new_eta.cloned()));
        *theta = DVector::from(Vec::from_iter(new_eta.map(|e| e.sigmoid() )));
        // theta.iter_mut().zip(eta.iter()).for_each(|(old, new)| *old = new.sigmoid() );
        self.update_log_partition();
        if let Some(ref mut suf) = self.suf_theta {
            *suf = Beta::sufficient_stat(self.theta.slice((0,0), (self.theta.nrows(),1)));
        }
        self.sampler.clear();
        for t in self.theta.iter() {
            self.sampler.push(rand_distr::Bernoulli::new(*t).unwrap());
        }

    }

    /*fn grad(&self, y : DMatrixSlice<'_, f64>, x : Option<DMatrix<f64>>) -> DVector<f64> {
        match &self.factor {
            BernoulliFactor::Fixed(mn) => {
                mn.grad(y, None)
            },
            _ => {
                unimplemented!()
            }
        }
    }*/

    fn set_parameter(&mut self, p : DVectorSlice<'_, f64>, natural : bool) {
        let eta = if natural {
            p.clone_owned()
        } else {
            Self::link(&p)
        };
        self.set_natural(&mut eta.iter());
    }

    fn view_parameter(&self, natural : bool) -> &DVector<f64> {
        match natural {
            true => &self.eta,
            false => &self.theta
        }
    }

    // fn observations(&self) -> Option<&DMatrix<f64>> {
    //    self.obs.as_ref()
    // }

    fn mean<'a>(&'a self) -> &'a DVector<f64> {
        &self.theta
    }

    fn mode(&self) -> DVector<f64> {
        self.theta.clone()
    }

    fn var(&self) -> DVector<f64> {
        self.theta.component_mul(&self.theta.map(|p| 1. - p))
    }

    fn cov(&self) -> Option<DMatrix<f64>> {
        None
    }

    fn cov_inv(&self) -> Option<DMatrix<f64>> {
        None
    }

    fn joint_log_prob(&self /*, y : DMatrixSlice<f64>, x : Option<DMatrixSlice<f64>>*/ ) -> Option<f64> {
        super::univariate_joint_log_prob(
            self.obs.as_ref(),
            self.factor.fixed_obs(),
            &self.factor,
            &self.view_parameter(true),
            &self.log_part,
            self.suf_theta.clone()
        )
    }

    fn sample_into(&self, mut dst : DMatrixSliceMut<'_,f64>) {
        use rand_distr::{Distribution};
        let opt_theta = sample_canonical_factor::<Bernoulli,_>(self.view_fixed_values(), &self.factor, self.n);
        let theta = opt_theta.as_ref().unwrap_or(&self.theta);
        for (i, _) in theta.iter().enumerate() {
            dst[(i,0)] = (self.sampler[i].sample(&mut rand::thread_rng()) as i32) as f64;
        }
    }
    
    fn dyn_factors(&self) -> (Option<&dyn Distribution>, Option<&dyn Distribution>) {
         (super::univariate_factor(&self.factor), None)
    }

    fn dyn_factors_mut<'a>(&'a mut self) -> (Option<&'a mut dyn Distribution>, Option<&'a mut dyn Distribution>) {
        (super::univariate_factor_mut(&mut self.factor), None)
    }

    //fn sample_into_transposed(&self, dst : DVectorSliceMut<'_, f64>) {
    //}

}

impl Markov for Bernoulli {

    fn natural_mut<'a>(&'a mut self) -> DVectorSliceMut<'a, f64> {
        self.eta.column_mut(0)
    }

    fn canonical_mut<'a>(&'a mut self) -> Option<DVectorSliceMut<'a, f64>> {
        Some(self.theta.column_mut(0))
    }

}

impl TryFrom<serde_json::Value> for Bernoulli {

    type Error = String;

    fn try_from(val : Value) -> Result<Self, String> {
        crate::model::parse::parse_univariate::<Bernoulli, Beta>(&val, "prop")
    }

}

impl TryFrom<Model> for Bernoulli {

    type Error = String;

    fn try_from(lik : Model) -> Result<Self, String> {
        match lik {
            Model::Bern(b) => Ok(b),
            _ => Err(format!("Object does not have a top-level bernoulli node"))
        }
    }

}

impl<'a> TryFrom<&'a Model> for &'a Bernoulli {

    type Error = String;

    fn try_from(lik : &'a Model) -> Result<Self, String> {
        match lik {
            Model::Bern(b) => Ok(b),
            _ => Err(format!("Object does not have a top-level bernoulli node"))
        }
    }
}

impl Observable for Bernoulli {

    fn observations(&mut self) -> &mut Option<DMatrix<f64>> {
        &mut self.obs
    }

    fn sample_size(&mut self) -> &mut usize {
        &mut self.n
    }

}

impl FixedConditional for Bernoulli {

    fn fixed_factor<'a>(&'a mut self) -> Option<&'a mut MultiNormal> {
        match &mut self.factor {
            UnivariateFactor::Fixed(ref mut mn) => {
                if mn.is_fixed() {
                    Some(mn)
                } else {
                    None
                }
            },
            _ => None
        }
    }

}

impl Into<serde_json::Value> for Bernoulli {

    fn into(mut self) -> serde_json::Value {
        let mut child = Map::new();
        if let Some(mut obs) = self.obs.take() {
            let obs_vec : Vec<f64> = obs.data.into();
            let obs_value : Value = obs_vec.into();
            child.insert(String::from("obs"), obs_value);
        }
        if let BernoulliFactor::Fixed(mn) = self.factor {
            let fv : Value = mn.into();
            child.insert(String::from("prop"), fv);
        } else {
            if let BernoulliFactor::Conjugate(beta) = self.factor {
                let bv : Value = beta.into();
                child.insert(String::from("prop"), bv);
            } else {
                child.insert(String::from("prop"), crate::model::vector_to_value(&self.theta));
            }
        }
        // let mut parent = Map::new();
        // parent.insert(String::from("bernoulli"), Value::Object(child));
        Value::Object(child)
    }

}

/*impl Predictive for Bernoulli {

    fn predict<'a>(&'a mut self, fixed : Option<&dyn Sample>) -> Option<&'a dyn Sample> {
        /*super::collect_fixed_if_required(self, fixed);
        let transf = |obs : &f64| -> bool { if *obs == 1.0 { true } else { false } };
        let preds = super::try_build_generalized_predictions(self, transf)
            .map_err(|e| println!("{}", e)).unwrap();
        self.sample = preds;
        self.view_prediction()*/
        unimplemented!()
    }

    fn view_prediction<'a>(&'a self) -> Option<&'a dyn Sample> {
        if self.sample.is_empty() {
            None
        } else {
            Some(&self.sample as &dyn Sample )
        }
    }

}*/

/*let linesearch = MoreThuenteLineSearch::new().c(1e-4, 0.9)?;
    let solver = LBFGS::new(linesearch, 7);

    let init_param = DVector::zeros(100);
    let res = Executor::new(model, solver, init_param)
        .add_observer(ArgminSlogLogger::term(), ObserverMode::Always)
        .max_iters(100)
        .run()
        .map_err(|e| format!("{}", e) )?;*/

/*impl Posterior for Bernoulli {

    fn dyn_factors_mut(&mut self) -> (Option<&mut dyn Posterior>, Option<&mut dyn Posterior>) {
        match &mut self.factor {
            BernoulliFactor::Conjugate(ref mut b) => (Some(b as &mut dyn Posterior), None),
            BernoulliFactor::Fixed(ref mut m) => (Some(m as &mut dyn Posterior), None),
            _ => (None, None)
        }
    }

    fn set_approximation(&mut self, m : MultiNormal) {
        unimplemented!()
    }

    fn approximate(&self) -> Option<&MultiNormal> {
        unimplemented!()
    }
}*/

/*impl Trajectory for Bernoulli {

    fn current<'a>(&'a self) -> Option<DVectorSlice<'a, f64>> {
        self.eta_traj.as_ref().and_then(|eta_traj| {
            Some(eta_traj.traj.column(eta_traj.pos))
        })
    }

    fn step_by<'a>(&'a mut self, diff_eta : DVectorSlice<'a, f64>, _update : bool) {
        self.eta_traj.as_mut().unwrap().step_increment(diff_eta);
    }

    fn step_to<'a>(&'a mut self, new_eta : Option<DVectorSlice<'a, f64>>, update : bool) {
        if let Some(ref mut eta_traj) = self.eta_traj {
            eta_traj.step(new_eta)
        } else {
            self.eta_traj = Some(EtaTrajectory::new(new_eta.unwrap()));
        }
        if update {
            self.set_parameter(new_eta.unwrap(), true);
        }
    }

    fn marginal(&self) -> Option<Sample> {
        self.eta_traj.as_ref().and_then(|eta_traj| {
            let cols : Vec<DVector<f64>> = eta_traj.traj.clone()
                .column_iter().take(eta_traj.pos)
                .map(|col| Self::link(&col) ).collect();
            let t_cols = DMatrix::from_columns(&cols[..]);
            Some(Sample::new(t_cols))
        })
    }

}*/

impl Default for Bernoulli {

    fn default() -> Self {
        Bernoulli {
            theta : DVector::from_element(1, 0.5),
            eta : DVector::from_element(1, 0.0),
            factor : BernoulliFactor::Empty,
            //eta_traj : None,
            sampler : Vec::new(),
            log_part : DVector::from_element(1, (2.).ln()),
            suf_theta : None,
            obs : None,
            // fixed_obs : None,
            name : None,
            fixed_names : None,
            n : 0,
            sample : HashMap::new()
        }
    }

}

impl Display for Bernoulli {

    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Bern({})", self.theta.nrows())
    }

}

/*impl Solver<DVector<f64>> for Bernoulli {

    const NAME : &'static str = "Bernoulli";

    // Defines the computations performed in a single iteration.
    fn next_iter(
        &mut self,
        op: &mut OpWrapper<DVector<f64>>,
        state: &IterState<O>
    ) -> Result<ArgminIterData<O>, argmin::core::Error> {
        /*// First we obtain the current parameter vector from the `state` struct (`x_k`).
        let xk = state.get_param();
        // Then we compute the gradient at `x_k` (`\nabla f(x_k)`)
        let grad = op.gradient(&xk)?;
        // Now subtract `\nabla f(x_k)` scaled by `omega` from `x_k` to compute `x_{k+1}`
        let xkp1 = xk.scaled_sub(&self.omega, &grad);
        // Return new paramter vector which will then be used by the `Executor` to update
        // `state`.
        Ok(ArgminIterData::new().param(xkp1))*/
        unimplemented!()
    }
} */

impl argmin::core::ArgminOp for Bernoulli {

    type Param = DVector<f64>;

    type Output = f64;

    type Hessian = ();

    type Jacobian = DVector<f64>;

    type Float = f64;

    fn apply(&self, p: &Self::Param) -> Result<Self::Output, argmin::core::Error> {
        // TODO Set parameter value p into the MultiNormal factor. The factor should be a RefCell
        // for that to be valid.
        Ok(self.joint_log_prob().unwrap())
    }

    fn jacobian(&self, p: &Self::Param) -> Result<Self::Jacobian, argmin::core::Error> {
        Ok(self.grad(self.obs.as_ref().unwrap().into(), None))
    }
}

